# Scaling × Feature Tier Interaction Effect

## Summary

A key finding from Phase 6C experiments: the optimal parameter budget depends on feature tier.

| Tier | Features | Optimal Budget (H1) | AUC |
|------|----------|---------------------|-----|
| a20 | 25 | 200M | 0.724 |
| a50 | 55 | 20M | 0.724 |

**Key insight**: Larger models (200M) perform best with fewer features (a20), while medium models (20M) perform best with more features (a50). This suggests a **regularization-capacity tradeoff** where additional features act as implicit regularization.

## Evidence

### AUC Comparison by Budget and Horizon

Data from threshold sweep experiments on validation set (2023-2024):

| Horizon | Budget | AUC (a20) | AUC (a50) | Delta |
|---------|--------|-----------|-----------|-------|
| H1 | 2M | 0.707 | 0.709 | +0.002 |
| H1 | 20M | 0.717 | **0.724** | +0.007 |
| H1 | 200M | **0.724** | 0.717 | -0.007 |
| H2 | 2M | 0.640 | 0.646 | +0.006 |
| H2 | 20M | 0.639 | 0.640 | +0.001 |
| H2 | 200M | **0.642** | **0.643** | +0.001 |
| H3 | 2M | 0.621 | 0.625 | +0.004 |
| H3 | 20M | 0.618 | 0.622 | +0.004 |
| H3 | 200M | **0.631** | **0.632** | +0.001 |
| H5 | 2M | 0.609 | 0.601 | -0.008 |
| H5 | 20M | 0.599 | **0.622** | +0.023 |
| H5 | 200M | **0.613** | 0.615 | +0.002 |

Bold indicates best budget within each tier-horizon combination.

### Optimal Budget Shift Pattern

For H1 (1-day direction prediction):
- **a20 (25 features)**: 200M optimal → Largest model benefits from limited feature space
- **a50 (55 features)**: 20M optimal → Medium model benefits from richer feature space

For longer horizons (H2-H5):
- Pattern less consistent, but 200M remains strong
- This suggests short-term prediction benefits most from scaling × feature interaction

## Interpretation

### Hypothesis: Feature-Induced Regularization

1. **With fewer features (a20)**: Models rely heavily on learning complex temporal patterns from limited inputs. Larger models (200M) can capture these patterns without overfitting because the feature space constrains the hypothesis space.

2. **With more features (a50)**: Additional features provide alternative signals that the model can leverage. Smaller models (20M) can generalize better because they're forced to use simple combinations of the richer feature set, while larger models may overfit to spurious feature correlations.

### Alternative Explanations

- **Feature redundancy**: a50 may contain redundant features that confuse larger models
- **Signal-to-noise ratio**: Additional features may add noise that larger models amplify
- **Training dynamics**: Larger models may require different hyperparameters for a50

### Supporting Evidence

The prediction ranges (min/max probabilities) support this interpretation:
- a20-200M: Wider prediction ranges, more confident predictions
- a50-200M: Slightly narrower ranges, suggesting more conservative predictions

## Implications for Practitioners

### Model Selection Guidelines

1. **Limited features + Large budget**: If your feature engineering is constrained, invest in larger models
2. **Rich features + Medium budget**: If you have many engineered features, a medium-sized model may generalize better
3. **Avoid**: Rich features + Large budget (highest overfitting risk)

### Feature Engineering Strategy

- **Quality over quantity**: 25 well-engineered features can match 55 features
- **Interaction awareness**: Consider how feature count interacts with model capacity
- **Validation-based tuning**: Always validate feature × budget combinations together

### Training Recommendations

When scaling features:
- Re-tune hyperparameters (especially dropout, weight decay)
- Consider reducing model size as you add features
- Monitor for signs of overfitting (train-val gap widening)

## Future Work

1. **Systematic feature ablation**: Which specific a50 features cause 200M degradation?
2. **Regularization tuning**: Can 200M match 20M on a50 with stronger regularization?
3. **Feature × Budget grid search**: Map the optimal frontier across feature counts

## Reproducibility

All data generated by:
```bash
# Run comparison analysis
python experiments/phase6c/compare_a20_a50_thresholds.py

# Generate figures
python experiments/phase6c/generate_scaling_figures.py
```

Output files:
- `outputs/phase6c/a20_vs_a50_comparison.json` - Full comparison data
- `outputs/phase6c/figures/scaling_feature_heatmap.png` - AUC improvement visualization
- `outputs/phase6c/figures/pr_curve_h{N}.png` - Precision-recall curves by horizon

## References

- Phase 6A results: `outputs/phase6a_final/threshold_sweep.csv`
- Phase 6C results: `outputs/phase6c/stage1_threshold_sweep.json`, `outputs/phase6c/comprehensive_threshold_sweep.json`
- Feature tier definitions: `docs/feature_engineering_exploration.md`
