# Architectural HPO search space for scaling experiments
#
# This config is used for Phase 6A architectural HPO where we search both
# model architecture (from arch_grid.py) AND training parameters.
#
# Key difference from default_search.yaml:
# - Architecture params (d_model, n_layers, n_heads, d_ff) come from pre-computed
#   valid architecture grid, not from this config
# - Training params use NARROWER ranges since we have fewer trials per architecture
# - Key is "training_search_space" (not "search_space") to distinguish purpose

# Study configuration
n_trials: 50
timeout_hours: null  # No timeout - rely on n_trials
direction: minimize  # Minimize validation loss

# Training parameter search space (narrower ranges for architectural HPO)
# Architecture params are sampled separately from arch_grid.py
# Note: batch_size removed - now determined dynamically by get_memory_safe_batch_config()
training_search_space:
  learning_rate:
    type: log_uniform
    low: 1.0e-4
    high: 1.0e-3

  epochs:
    type: categorical
    choices: [50, 75, 100]

  weight_decay:
    type: log_uniform
    low: 1.0e-4
    high: 5.0e-3

  warmup_steps:
    type: categorical
    choices: [100, 200, 300, 500]

  dropout:
    type: uniform
    low: 0.1
    high: 0.3

# Early stopping configuration
early_stopping:
  patience: 10
  min_delta: 0.001
